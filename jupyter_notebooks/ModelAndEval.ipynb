{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **Modelling and Evaluation Notebook**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "* Write here your notebook objective, for example, \"Fetch data from Kaggle and save as raw data\", or \"engineer features for modelling\"\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* Write here which data or information you need to run the notebook \n",
        "\n",
        "## Outputs\n",
        "\n",
        "* Write here which files, code or artefacts you generate by the end of the notebook \n",
        "\n",
        "## Additional Comments\n",
        "\n",
        "* In case you have any additional comments that don't fit in the previous bullets, please state them here. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Initial steps plan EDIT:\n",
        "\n",
        "Import packages\n",
        "cwd\n",
        "set input dir\n",
        "set output dir\n",
        "set lables ?\n",
        "no of img for train, test and val\n",
        "image aug (image data generator) on TRAIN set, then validation and test\n",
        "plot aug train images\n",
        "\n",
        "plot aug val and test images\n",
        "save class indices\n",
        "\n",
        "___\n",
        "\n",
        "model creation \n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense, Conv2D, MaxPooling2D\n",
        "\n",
        "OPTION 1 starter:\n",
        "\n",
        "def create_tf_model():\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Conv2D(filters=32, kernel_size=(3, 3),  # find only dominant pixels \n",
        "              input_shape=image_shape, activation='relu', ))   # uses average image shape from the data visualtion stage \n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))  # reduce size of image by extracting only those dominant features\n",
        "\n",
        "\n",
        "    model.add(Conv2D(filters=64, kernel_size=(3, 3),\n",
        "              input_shape=image_shape, activation='relu', ))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    model.add(Conv2D(filters=64, kernel_size=(3, 3),\n",
        "              input_shape=image_shape, activation='relu', ))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    model.add(Flatten())  # single list of all values, to feed into dense layer\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "\n",
        "    model.add(Dropout(0.5)) # drop 50% to avoid overfitting \n",
        "    model.add(Dense(1, activation='sigmoid'))  # output layer (only 1 neuron) with probability result\n",
        "\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "___\n",
        "OPTION 2 starter:\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "def create_powdery_mildew_classifier(input_shape=(224, 224, 3)):\n",
        "    \"\"\"\n",
        "    Creates a CNN model for classifying cherry leaves as healthy or infected with powdery mildew.\n",
        "    \n",
        "    Args:\n",
        "        input_shape: Tuple of (height, width, channels) for input images\n",
        "        \n",
        "    Returns:\n",
        "        tf.keras.Model: Compiled CNN model\n",
        "    \"\"\"\n",
        "    model = models.Sequential([\n",
        "        # Input Layer and Initial Convolution Block\n",
        "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        \n",
        "        # Second Convolution Block\n",
        "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        \n",
        "        # Third Convolution Block\n",
        "        layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        \n",
        "        # Fourth Convolution Block\n",
        "        layers.Conv2D(256, (3, 3), activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        \n",
        "        # Flatten and Dense Layers\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(256, activation='relu'),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(1, activation='sigmoid')  # Binary classification\n",
        "    ])\n",
        "    \n",
        "    # Compile model\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
        "    )\n",
        "    \n",
        "    return model\n",
        "\n",
        "# Example usage:\n",
        "# model = create_powdery_mildew_classifier()\n",
        "# model.summary()\n",
        "\n",
        "\n",
        "\n",
        "___\n",
        "\n",
        "Get model summary \n",
        "early stopping \n",
        "fit model for model training \n",
        "25 epochs\n",
        "save model as .h5 file\n",
        "\n",
        "___\n",
        "\n",
        "\n",
        "check model performanc \n",
        "- learning curve (loss, accuracy)\n",
        "- save eval pkl\n",
        "model eval: \n",
        "- load model\n",
        "predict on new data\n",
        "\n",
        "push files to repo\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "POSSIBLE MODEL ENHANCEMENTS AFTER:\n",
        "- edge computing?\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "def create_powdery_mildew_classifier(input_shape=(224, 224, 3)):\n",
        "    \"\"\"\n",
        "    Creates a CNN model for classifying cherry leaves as healthy or infected with powdery mildew.\n",
        "    Includes comments for hyperparameter tuning options.\n",
        "    \n",
        "    Args:\n",
        "        input_shape: Tuple of (height, width, channels) for input images\n",
        "        \n",
        "    Returns:\n",
        "        tf.keras.Model: Compiled CNN model\n",
        "    \"\"\"\n",
        "    model = models.Sequential([\n",
        "        # Input Layer and Initial Convolution Block\n",
        "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
        "        # Alternative options:\n",
        "        # - filters: try [16, 48, 64] for different feature map sizes\n",
        "        # - kernel_size: try (5,5) or (7,7) for larger feature detection\n",
        "        # - activation: 'leaky_relu', 'elu', or 'selu' for different learning dynamics\n",
        "        # - Add padding='same' to maintain spatial dimensions\n",
        "        # - Add strides=(2,2) to reduce spatial dimensions without MaxPooling\n",
        "        \n",
        "        layers.BatchNormalization(),\n",
        "        # Alternative options:\n",
        "        # - momentum: try [0.8, 0.9, 0.99] for different update speeds\n",
        "        # - epsilon: default is 1e-3, try 1e-5 for different numerical stability\n",
        "        # - Consider removing BatchNorm if using dropout after conv layers\n",
        "        \n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        # Alternative options:\n",
        "        # - pool_size: try (3,3) for more aggressive downsampling\n",
        "        # - strides: try (1,1) for more gradual dimension reduction\n",
        "        # - Consider AveragePooling2D instead for different feature aggregation\n",
        "        \n",
        "        # Second Convolution Block\n",
        "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "        # Alternative options:\n",
        "        # - filters: try [32, 96, 128] for different capacity\n",
        "        # - Add L1/L2 regularization: kernel_regularizer=tf.keras.regularizers.l2(0.01)\n",
        "        # - Try spatial dropout: layers.SpatialDropout2D(0.2)\n",
        "        \n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        \n",
        "        # Third Convolution Block\n",
        "        layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "        # Alternative options:\n",
        "        # - filters: try [64, 256] depending on model capacity needs\n",
        "        # - Try adding dilation_rate=(2,2) for larger receptive field\n",
        "        # - Consider adding residual connections for deeper architectures\n",
        "        \n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        \n",
        "        # Fourth Convolution Block\n",
        "        layers.Conv2D(256, (3, 3), activation='relu'),\n",
        "        # Alternative options:\n",
        "        # - filters: try [128, 512] for different model capacities\n",
        "        # - Consider adding squeeze-and-excitation blocks for attention\n",
        "        # - Try grouped convolutions for efficiency\n",
        "        \n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        \n",
        "        # Flatten and Dense Layers\n",
        "        layers.Flatten(),\n",
        "        # Alternative options:\n",
        "        # - Consider GlobalAveragePooling2D instead of Flatten\n",
        "        # - Try GlobalMaxPooling2D for different feature aggregation\n",
        "        \n",
        "        layers.Dense(512, activation='relu'),\n",
        "        # Alternative options:\n",
        "        # - units: try [256, 1024] for different capacities\n",
        "        # - activation: try 'swish' or 'mish' for potentially better performance\n",
        "        # - Add kernel_regularizer for weight regularization\n",
        "        \n",
        "        layers.Dropout(0.5),\n",
        "        # Alternative options:\n",
        "        # - rate: try [0.3, 0.4, 0.6] for different regularization strengths\n",
        "        # - Consider using GaussianDropout for different dropout behavior\n",
        "        \n",
        "        layers.Dense(256, activation='relu'),\n",
        "        # Alternative options:\n",
        "        # - Consider removing this layer for a simpler model\n",
        "        # - Try adding batch normalization before activation\n",
        "        # - Add activity_regularizer for activation sparsity\n",
        "        \n",
        "        layers.Dropout(0.3),\n",
        "        \n",
        "        layers.Dense(1, activation='sigmoid')  # Binary classification\n",
        "        # Alternative options:\n",
        "        # - Try 'linear' activation with from_logits=True in loss function\n",
        "        # - Consider threshold tuning for different precision/recall trade-offs\n",
        "    ])\n",
        "    \n",
        "    # Compile options\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        # Alternative optimizers:\n",
        "        # - SGD with momentum: optimizers.SGD(lr=0.01, momentum=0.9)\n",
        "        # - RMSprop: optimizers.RMSprop(lr=0.001, rho=0.9)\n",
        "        # - AdamW: optimizers.AdamW(lr=0.001, weight_decay=0.004)\n",
        "        \n",
        "        loss='binary_crossentropy',\n",
        "        # Alternative losses:\n",
        "        # - BinaryCrossentropy(from_logits=True) if using linear activation\n",
        "        # - focal_loss for handling class imbalance\n",
        "        # - Consider custom loss functions for specific requirements\n",
        "        \n",
        "        metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
        "        # Additional metrics to consider:\n",
        "        # - AUC for ROC curve analysis\n",
        "        # - F1Score for balanced precision/recall\n",
        "        # - SpecificityAtSensitivity for medical-style evaluation\n",
        "    )\n",
        "    \n",
        "    return model\n",
        "\n",
        "Training options to consider:\n",
        "- batch_size: try [16, 32, 64, 128]\n",
        "- learning_rate scheduling: reduce_lr_on_plateau or cosine decay\n",
        "- early stopping patience: try [5, 10, 15] epochs\n",
        "- validation_split: try [0.1, 0.2, 0.3]\n",
        "- class_weights for imbalanced dataset\n",
        "\n",
        "Example usage with different input sizes:\n",
        "\n",
        "Smaller input:\n",
        "model = create_powdery_mildew_classifier(input_shape=(160, 160, 3))\n",
        "\n",
        "Larger input:\n",
        "model = create_powdery_mildew_classifier(input_shape=(299, 299, 3))   \n",
        "\n",
        "\n",
        "\n",
        "Systematic Tuning Approach:\n",
        "\n",
        "Start with architecture modifications (layer sizes, depth)\n",
        "Then tune regularization parameters (dropout, batch norm)\n",
        "Finally adjust optimization parameters (learning rate, batch size)\n",
        "\n",
        "\n",
        "Cross-Validation Strategy:\n",
        "pythonCopyfrom sklearn.model_selection import KFold\n",
        "k_fold = KFold(n_splits=5, shuffle=True)\n",
        "# Use this to validate hyperparameter choices\n",
        "\n",
        "Monitoring Tips:\n",
        "\n",
        "Watch validation loss for overfitting\n",
        "Monitor precision/recall trade-offs\n",
        "Use TensorBoard for visualization of metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "# Change working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We are assuming you will store the notebooks in a subfolder, therefore when running the notebook in the editor, you will need to change the working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOGIGS-uz3i2"
      },
      "source": [
        "We need to change the working directory from its current folder to its parent folder\n",
        "* We access the current directory with os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MWW8E7lz3i7"
      },
      "source": [
        "We want to make the parent of the current directory the new current directory\n",
        "* os.path.dirname() gets the parent directory\n",
        "* os.chir() defines the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwHsQRWjz3i9",
        "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c"
      },
      "outputs": [],
      "source": [
        "os.chdir(os.path.dirname(current_dir))\n",
        "print(\"You set a new current directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_xPk_Ijz3i-"
      },
      "source": [
        "Confirm the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vz3S-_kjz3jA",
        "outputId": "00b79ae4-75d0-4a96-d193-ac9ef9847ea2"
      },
      "outputs": [],
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "# Section 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Section 1 content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY3l0-AxO93d"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFQo3ycuO-v6"
      },
      "source": [
        "# Section 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Section 2 content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "NOTE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* You may add as many sections as you want, as long as it supports your project workflow.\n",
        "* All notebook's cells should be run top-down (you can't create a dynamic wherein a given point you need to go back to a previous cell to execute some task, like go back to a previous cell and refresh a variable content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltNetd085qHf"
      },
      "source": [
        "# Push files to Repo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* If you don't need to push files to Repo, you may replace this section with \"Conclusions and Next Steps\" and state your conclusions and next steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKlnIozA4eQO",
        "outputId": "fd09bc1f-adb1-4511-f6ce-492a6af570c0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "try:\n",
        "    # create here your folder\n",
        "    # os.makedirs(name='')\n",
        "except Exception as e:\n",
        "    print(e)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "8b8334dab9339717f727a1deaf837b322d7a41c20d15cc86be99a8e69ceec8ce"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 64-bit ('3.8.12': pyenv)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
