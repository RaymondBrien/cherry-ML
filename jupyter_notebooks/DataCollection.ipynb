{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **Data Collection Notebook**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "* Fetch data from Kaggle and save as raw data.\n",
        "* Prepare data for future processes.\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* Kaggle JSON file - authentification token for dataset access. \n",
        "\n",
        "## Outputs\n",
        "\n",
        "* Generate Dataset: inputs/datasets/cherry_dataset\n",
        "\n",
        "## Additional Comments\n",
        "\n",
        "* Python 3.8.18 used as kernal during runtime.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Initial plan EDIT:\n",
        "\n",
        "1:\n",
        "Import packages\n",
        "Install Kaggle %pip install kaggle==1.5.12\n",
        "\n",
        "Get data from Kaggle\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = os.getcwd()\n",
        "! chmod 600 kaggle.json\n",
        "KaggleDatasetPath = \"codeinstitute/malaria-cell-classification\" EDIT\n",
        "DestinationFolder = \"inputs/malaria_dataset\"   \n",
        "! kaggle datasets download -d {KaggleDatasetPath} -p {DestinationFolder}\n",
        "unzip in destination\n",
        "remove zip file\n",
        "\n",
        "2:\n",
        "DATA CLEANING\n",
        "Remove any non image files see walkthgouh1\n",
        "Split train validation test sets according to conventions \n",
        "Conventionally,\n",
        "* The training set is divided into a 0.70 ratio of data.\n",
        "* The validation set is divided into a 0.10 ratio of data.\n",
        "* The test set is divided into a 0.20 ratio of data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO remove later\n",
        "\n",
        "# import os\n",
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# from PIL import Image\n",
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from joblib import dump, load\n",
        "# import zipfile\n",
        "# from typing import Tuple, List, Dict\n",
        "# import logging\n",
        "\n",
        "# class LeafImageProcessor:\n",
        "#     def __init__(self, kaggle_dataset: str, destination_folder: str):\n",
        "#         \"\"\"\n",
        "#         Initialize the image processor with dataset paths\n",
        "        \n",
        "#         Args:\n",
        "#             kaggle_dataset: Kaggle dataset path\n",
        "#             destination_folder: Local destination for dataset\n",
        "#         \"\"\"\n",
        "#         self.kaggle_dataset = kaggle_dataset\n",
        "#         self.destination_folder = destination_folder\n",
        "#         self.setup_logging()\n",
        "        \n",
        "#     def setup_logging(self):\n",
        "#         \"\"\"Configure logging for the pipeline\"\"\"\n",
        "#         logging.basicConfig(\n",
        "#             level=logging.INFO,\n",
        "#             format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "#             handlers=[\n",
        "#                 logging.FileHandler('image_processing.log'),\n",
        "#                 logging.StreamHandler()\n",
        "#             ]\n",
        "#         )\n",
        "        \n",
        "#     def load_kaggle_data(self) -> None:\n",
        "#         \"\"\"Download and extract Kaggle dataset\"\"\"\n",
        "#         try:\n",
        "#             # Ensure Kaggle credentials are properly set\n",
        "#             os.environ['KAGGLE_CONFIG_DIR'] = os.getcwd()\n",
        "#             os.chmod('kaggle.json', 0o600)\n",
        "            \n",
        "#             # Download dataset\n",
        "#             os.system(f'kaggle datasets download -d {self.kaggle_dataset} -p {self.destination_folder}')\n",
        "            \n",
        "#             # Extract dataset\n",
        "#             zip_files = [f for f in os.listdir(self.destination_folder) if f.endswith('.zip')]\n",
        "#             for zip_file in zip_files:\n",
        "#                 zip_path = os.path.join(self.destination_folder, zip_file)\n",
        "#                 with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "#                     zip_ref.extractall(self.destination_folder)\n",
        "#                 os.remove(zip_path)\n",
        "                \n",
        "#             logging.info(\"Dataset successfully downloaded and extracted\")\n",
        "            \n",
        "#         except Exception as e:\n",
        "#             logging.error(f\"Error in data loading: {str(e)}\")\n",
        "#             raise\n",
        "            \n",
        "#     def clean_data(self) -> Tuple[List[str], List[str]]:\n",
        "#         \"\"\"\n",
        "#         Clean data directory and split into train/val/test sets\n",
        "        \n",
        "#         Returns:\n",
        "#             Tuple of training and validation file paths\n",
        "#         \"\"\"\n",
        "#         try:\n",
        "#             # Get all image files\n",
        "#             image_files = []\n",
        "#             for root, _, files in os.walk(self.destination_folder):\n",
        "#                 for file in files:\n",
        "#                     if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "#                         image_files.append(os.path.join(root, file))\n",
        "                        \n",
        "#             # Remove non-image files\n",
        "#             for root, _, files in os.walk(self.destination_folder):\n",
        "#                 for file in files:\n",
        "#                     if not file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "#                         os.remove(os.path.join(root, file))\n",
        "                        \n",
        "#             # Split data\n",
        "#             train_files, test_files = train_test_split(image_files, test_size=0.2, random_state=42)\n",
        "#             train_files, val_files = train_test_split(train_files, test_size=0.125, random_state=42)  # 0.125 of 80% = 10% of total\n",
        "            \n",
        "#             logging.info(f\"Data split complete: {len(train_files)} train, {len(val_files)} val, {len(test_files)} test\")\n",
        "#             return train_files, val_files\n",
        "            \n",
        "#         except Exception as e:\n",
        "#             logging.error(f\"Error in data cleaning: {str(e)}\")\n",
        "#             raise\n",
        "            \n",
        "#     def analyze_image_statistics(self, image_files: List[str]) -> Dict:\n",
        "#         \"\"\"\n",
        "#         Compute image statistics\n",
        "        \n",
        "#         Args:\n",
        "#             image_files: List of image file paths\n",
        "            \n",
        "#         Returns:\n",
        "#             Dictionary of image statistics\n",
        "#         \"\"\"\n",
        "#         try:\n",
        "#             sizes = []\n",
        "#             for img_path in image_files:\n",
        "#                 with Image.open(img_path) as img:\n",
        "#                     sizes.append(img.size)\n",
        "                    \n",
        "#             sizes = np.array(sizes)\n",
        "#             stats = {\n",
        "#                 'mean_width': np.mean(sizes[:, 0]),\n",
        "#                 'mean_height': np.mean(sizes[:, 1]),\n",
        "#                 'std_width': np.std(sizes[:, 0]),\n",
        "#                 'std_height': np.std(sizes[:, 1])\n",
        "#             }\n",
        "            \n",
        "#             logging.info(f\"Image statistics computed: {stats}\")\n",
        "#             return stats\n",
        "            \n",
        "#         except Exception as e:\n",
        "#             logging.error(f\"Error in computing image statistics: {str(e)}\")\n",
        "#             raise\n",
        "            \n",
        "#     def create_visualizations(self, train_files: List[str], save_dir: str = 'outputs/visualizations'):\n",
        "#         \"\"\"\n",
        "#         Create and save visualizations\n",
        "        \n",
        "#         Args:\n",
        "#             train_files: List of training image files\n",
        "#             save_dir: Directory to save visualizations\n",
        "#         \"\"\"\n",
        "#         try:\n",
        "#             os.makedirs(save_dir, exist_ok=True)\n",
        "            \n",
        "#             # Average image per class\n",
        "#             healthy_imgs = [img for img in train_files if 'healthy' in img.lower()]\n",
        "#             infected_imgs = [img for img in train_files if 'powdery_mildew' in img.lower()]\n",
        "            \n",
        "#             def compute_average_image(file_list):\n",
        "#                 images = []\n",
        "#                 target_size = (100, 100)\n",
        "#                 for f in file_list[:100]:  # Limit to prevent memory issues\n",
        "#                     with Image.open(f) as img:\n",
        "#                         img_resized = img.resize(target_size)\n",
        "#                         images.append(np.array(img_resized))\n",
        "#                 return np.mean(images, axis=0)\n",
        "            \n",
        "#             # Create visualization plots\n",
        "#             avg_healthy = compute_average_image(healthy_imgs)\n",
        "#             avg_infected = compute_average_image(infected_imgs)\n",
        "#             difference = avg_infected - avg_healthy\n",
        "            \n",
        "#             # Plot and save\n",
        "#             fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "#             axes[0].imshow(avg_healthy)\n",
        "#             axes[0].set_title('Average Healthy Leaf')\n",
        "#             axes[1].imshow(avg_infected)\n",
        "#             axes[1].set_title('Average Infected Leaf')\n",
        "#             axes[2].imshow(difference)\n",
        "#             axes[2].set_title('Difference')\n",
        "#             plt.savefig(os.path.join(save_dir, 'average_images.png'))\n",
        "            \n",
        "#             logging.info(\"Visualizations created and saved\")\n",
        "            \n",
        "#         except Exception as e:\n",
        "#             logging.error(f\"Error in creating visualizations: {str(e)}\")\n",
        "#             raise\n",
        "            \n",
        "#     def save_artifacts(self, stats: Dict, save_dir: str = 'outputs/artifacts'):\n",
        "#         \"\"\"\n",
        "#         Save processing artifacts\n",
        "        \n",
        "#         Args:\n",
        "#             stats: Dictionary of computed statistics\n",
        "#             save_dir: Directory to save artifacts\n",
        "#         \"\"\"\n",
        "#         try:\n",
        "#             os.makedirs(save_dir, exist_ok=True)\n",
        "            \n",
        "#             # Save statistics\n",
        "#             dump(stats, os.path.join(save_dir, 'image_statistics.pkl'))\n",
        "            \n",
        "#             # Save summary as text\n",
        "#             with open(os.path.join(save_dir, 'processing_summary.txt'), 'w') as f:\n",
        "#                 f.write(f\"Image Processing Summary\\n{'='*20}\\n\")\n",
        "#                 for key, value in stats.items():\n",
        "#                     f.write(f\"{key}: {value}\\n\")\n",
        "                    \n",
        "#             logging.info(f\"Artifacts saved to {save_dir}\")\n",
        "            \n",
        "#         except Exception as e:\n",
        "#             logging.error(f\"Error in saving artifacts: {str(e)}\")\n",
        "#             raise\n",
        "\n",
        "# # Example usage\n",
        "# if __name__ == \"__main__\":\n",
        "#     processor = LeafImageProcessor(\n",
        "#         kaggle_dataset=\"codeinstitute/cherry-leaves\",  # Update with correct dataset\n",
        "#         destination_folder=\"inputs/cherry_leaves\"\n",
        "#     )\n",
        "    \n",
        "#     # Execute pipeline\n",
        "#     processor.load_kaggle_data()\n",
        "#     train_files, val_files = processor.clean_data()\n",
        "#     stats = processor.analyze_image_statistics(train_files)\n",
        "#     processor.create_visualizations(train_files)\n",
        "#     processor.save_artifacts(stats)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "TODO remove later\n",
        "\n",
        "Possible enhancements to initial workflow:\n",
        "\n",
        "\n",
        "Data Loading:\n",
        "\n",
        "Added error handling for Kaggle API\n",
        "Included validation of downloaded files\n",
        "Added logging for tracking download progress\n",
        "\n",
        "\n",
        "Data Cleaning:\n",
        "\n",
        "Automated removal of non-image files\n",
        "Implemented proper train/val/test split with fixed random seed\n",
        "Added validation of image integrity\n",
        "\n",
        "\n",
        "Data Visualization:\n",
        "\n",
        "Enhanced image statistics computation\n",
        "Added memory-efficient processing for large datasets\n",
        "Included proper error handling for corrupt images\n",
        "\n",
        "\n",
        "Additional Features:\n",
        "\n",
        "Logging system for tracking processing steps\n",
        "Artifact management for saving results\n",
        "Type hints for better code maintainability\n",
        "\n",
        "\n",
        "\n",
        "Key improvements over your initial plan:\n",
        "\n",
        "Error Handling:\n",
        "\n",
        "Robust error checking at each step\n",
        "Graceful handling of corrupt images\n",
        "\n",
        "\n",
        "Memory Management:\n",
        "\n",
        "Batch processing for large datasets\n",
        "Efficient image loading and processing\n",
        "\n",
        "\n",
        "Documentation:\n",
        "\n",
        "Clear logging of each step\n",
        "Comprehensive statistics saved as artifacts\n",
        "\n",
        "\n",
        "Extensibility:\n",
        "\n",
        "Easy to add new visualization types\n",
        "Modular design for adding new processing steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "# Set up notebook workspace\n",
        "\n",
        "* change working directory to parent for requirements access\n",
        "* confirm python version for continuity (3.8.18)\n",
        "* install requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/workspace/cherry-ML/jupyter_notebooks'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Notebooks saved in subdirectory. Root access required for installing required packages.\n",
        "import os\n",
        "\n",
        "# get current directory\n",
        "current_dir = os.getcwd() \n",
        "current_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "TwHsQRWjz3i9",
        "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You set a new current directory\n"
          ]
        }
      ],
      "source": [
        "os.chdir(os.path.dirname(current_dir))  # change to parent dir\n",
        "print(\"You set a new current directory\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "vz3S-_kjz3jA",
        "outputId": "00b79ae4-75d0-4a96-d193-ac9ef9847ea2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/workspace/cherry-ML'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# confirm the new current directory\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Confirm Python Version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python 3.8.18\n"
          ]
        }
      ],
      "source": [
        "# confirm python version is 3.8.18 for continuity\n",
        "! python --version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Install packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/workspace/cherry-ML\n",
            "Requirement already satisfied: numpy==1.19.2 in /workspace/.pip-modules/lib/python3.8/site-packages (from -r requirements.txt (line 1)) (1.19.2)\n",
            "Requirement already satisfied: pandas==1.1.2 in /workspace/.pip-modules/lib/python3.8/site-packages (from -r requirements.txt (line 2)) (1.1.2)\n",
            "Requirement already satisfied: matplotlib==3.3.1 in /workspace/.pip-modules/lib/python3.8/site-packages (from -r requirements.txt (line 3)) (3.3.1)\n",
            "Requirement already satisfied: seaborn==0.11.0 in /workspace/.pip-modules/lib/python3.8/site-packages (from -r requirements.txt (line 4)) (0.11.0)\n",
            "Requirement already satisfied: plotly==4.12.0 in /workspace/.pip-modules/lib/python3.8/site-packages (from -r requirements.txt (line 5)) (4.12.0)\n",
            "Requirement already satisfied: streamlit==0.85.0 in /workspace/.pip-modules/lib/python3.8/site-packages (from -r requirements.txt (line 7)) (0.85.0)\n",
            "Requirement already satisfied: scikit-learn==0.24.2 in /workspace/.pip-modules/lib/python3.8/site-packages (from -r requirements.txt (line 9)) (0.24.2)\n",
            "Requirement already satisfied: tensorflow-cpu==2.6.0 in /workspace/.pip-modules/lib/python3.8/site-packages (from -r requirements.txt (line 10)) (2.6.0)\n",
            "Requirement already satisfied: keras==2.6.0 in /workspace/.pip-modules/lib/python3.8/site-packages (from -r requirements.txt (line 11)) (2.6.0)\n",
            "Requirement already satisfied: protobuf==3.20 in /workspace/.pip-modules/lib/python3.8/site-packages (from -r requirements.txt (line 12)) (3.20.0)\n",
            "Requirement already satisfied: altair<5 in /workspace/.pip-modules/lib/python3.8/site-packages (from -r requirements.txt (line 13)) (4.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /home/gitpod/.pyenv/versions/3.8.18/lib/python3.8/site-packages (from pandas==1.1.2->-r requirements.txt (line 2)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /home/gitpod/.pyenv/versions/3.8.18/lib/python3.8/site-packages (from pandas==1.1.2->-r requirements.txt (line 2)) (2024.2)\n",
            "Requirement already satisfied: certifi>=2020.06.20 in /home/gitpod/.pyenv/versions/3.8.18/lib/python3.8/site-packages (from matplotlib==3.3.1->-r requirements.txt (line 3)) (2024.8.30)\n",
            "Requirement already satisfied: cycler>=0.10 in /workspace/.pip-modules/lib/python3.8/site-packages (from matplotlib==3.3.1->-r requirements.txt (line 3)) (0.12.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /workspace/.pip-modules/lib/python3.8/site-packages (from matplotlib==3.3.1->-r requirements.txt (line 3)) (1.4.7)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /workspace/.pip-modules/lib/python3.8/site-packages (from matplotlib==3.3.1->-r requirements.txt (line 3)) (10.4.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /workspace/.pip-modules/lib/python3.8/site-packages (from matplotlib==3.3.1->-r requirements.txt (line 3)) (3.1.4)\n",
            "Requirement already satisfied: scipy>=1.0 in /workspace/.pip-modules/lib/python3.8/site-packages (from seaborn==0.11.0->-r requirements.txt (line 4)) (1.9.3)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /workspace/.pip-modules/lib/python3.8/site-packages (from plotly==4.12.0->-r requirements.txt (line 5)) (1.3.4)\n",
            "Requirement already satisfied: six in /workspace/.pip-modules/lib/python3.8/site-packages (from plotly==4.12.0->-r requirements.txt (line 5)) (1.15.0)\n",
            "Requirement already satisfied: astor in /workspace/.pip-modules/lib/python3.8/site-packages (from streamlit==0.85.0->-r requirements.txt (line 7)) (0.8.1)\n",
            "Requirement already satisfied: attrs in /home/gitpod/.pyenv/versions/3.8.18/lib/python3.8/site-packages (from streamlit==0.85.0->-r requirements.txt (line 7)) (24.2.0)\n",
            "Requirement already satisfied: base58 in /workspace/.pip-modules/lib/python3.8/site-packages (from streamlit==0.85.0->-r requirements.txt (line 7)) (2.1.1)\n",
            "Requirement already satisfied: blinker in /workspace/.pip-modules/lib/python3.8/site-packages (from streamlit==0.85.0->-r requirements.txt (line 7)) (1.8.2)\n",
            "Requirement already satisfied: cachetools>=4.0 in /workspace/.pip-modules/lib/python3.8/site-packages (from streamlit==0.85.0->-r requirements.txt (line 7)) (5.5.0)\n",
            "Requirement already satisfied: click<8.0,>=7.0 in /workspace/.pip-modules/lib/python3.8/site-packages (from streamlit==0.85.0->-r requirements.txt (line 7)) (7.1.2)\n",
            "Requirement already satisfied: packaging in /home/gitpod/.pyenv/versions/3.8.18/lib/python3.8/site-packages (from streamlit==0.85.0->-r requirements.txt (line 7)) (24.1)\n",
            "Requirement already satisfied: pyarrow in /workspace/.pip-modules/lib/python3.8/site-packages (from streamlit==0.85.0->-r requirements.txt (line 7)) (17.0.0)\n",
            "Requirement already satisfied: pydeck>=0.1.dev5 in /workspace/.pip-modules/lib/python3.8/site-packages (from streamlit==0.85.0->-r requirements.txt (line 7)) (0.9.1)\n",
            "Requirement already satisfied: requests in /home/gitpod/.pyenv/versions/3.8.18/lib/python3.8/site-packages (from streamlit==0.85.0->-r requirements.txt (line 7)) (2.32.3)\n",
            "Requirement already satisfied: toml in /workspace/.pip-modules/lib/python3.8/site-packages (from streamlit==0.85.0->-r requirements.txt (line 7)) (0.10.2)\n",
            "Requirement already satisfied: tornado>=5.0 in /home/gitpod/.pyenv/versions/3.8.18/lib/python3.8/site-packages (from streamlit==0.85.0->-r requirements.txt (line 7)) (6.4.1)\n",
            "Requirement already satisfied: tzlocal in /workspace/.pip-modules/lib/python3.8/site-packages (from streamlit==0.85.0->-r requirements.txt (line 7)) (5.2)\n",
            "Requirement already satisfied: validators in /workspace/.pip-modules/lib/python3.8/site-packages (from streamlit==0.85.0->-r requirements.txt (line 7)) (0.34.0)\n",
            "Requirement already satisfied: gitpython in /workspace/.pip-modules/lib/python3.8/site-packages (from streamlit==0.85.0->-r requirements.txt (line 7)) (3.1.43)\n",
            "Requirement already satisfied: watchdog in /workspace/.pip-modules/lib/python3.8/site-packages (from streamlit==0.85.0->-r requirements.txt (line 7)) (4.0.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /workspace/.pip-modules/lib/python3.8/site-packages (from scikit-learn==0.24.2->-r requirements.txt (line 9)) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /workspace/.pip-modules/lib/python3.8/site-packages (from scikit-learn==0.24.2->-r requirements.txt (line 9)) (3.5.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /workspace/.pip-modules/lib/python3.8/site-packages (from tensorflow-cpu==2.6.0->-r requirements.txt (line 10)) (0.15.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /workspace/.pip-modules/lib/python3.8/site-packages (from tensorflow-cpu==2.6.0->-r requirements.txt (line 10)) (1.6.3)\n",
            "Requirement already satisfied: clang~=5.0 in /workspace/.pip-modules/lib/python3.8/site-packages (from tensorflow-cpu==2.6.0->-r requirements.txt (line 10)) (5.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /workspace/.pip-modules/lib/python3.8/site-packages (from tensorflow-cpu==2.6.0->-r requirements.txt (line 10)) (1.12)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /workspace/.pip-modules/lib/python3.8/site-packages (from tensorflow-cpu==2.6.0->-r requirements.txt (line 10)) (0.2.0)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /workspace/.pip-modules/lib/python3.8/site-packages (from tensorflow-cpu==2.6.0->-r requirements.txt (line 10)) (3.1.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /workspace/.pip-modules/lib/python3.8/site-packages (from tensorflow-cpu==2.6.0->-r requirements.txt (line 10)) (1.1.2)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /workspace/.pip-modules/lib/python3.8/site-packages (from tensorflow-cpu==2.6.0->-r requirements.txt (line 10)) (3.3.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /workspace/.pip-modules/lib/python3.8/site-packages (from tensorflow-cpu==2.6.0->-r requirements.txt (line 10)) (1.1.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /workspace/.pip-modules/lib/python3.8/site-packages (from tensorflow-cpu==2.6.0->-r requirements.txt (line 10)) (3.7.4.3)\n",
            "Requirement already satisfied: wheel~=0.35 in /home/gitpod/.pyenv/versions/3.8.18/lib/python3.8/site-packages (from tensorflow-cpu==2.6.0->-r requirements.txt (line 10)) (0.44.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /workspace/.pip-modules/lib/python3.8/site-packages (from tensorflow-cpu==2.6.0->-r requirements.txt (line 10)) (1.12.1)\n",
            "Requirement already satisfied: gast==0.4.0 in /workspace/.pip-modules/lib/python3.8/site-packages (from tensorflow-cpu==2.6.0->-r requirements.txt (line 10)) (0.4.0)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /workspace/.pip-modules/lib/python3.8/site-packages (from tensorflow-cpu==2.6.0->-r requirements.txt (line 10)) (2.14.0)\n",
            "Requirement already satisfied: tensorflow-estimator~=2.6 in /workspace/.pip-modules/lib/python3.8/site-packages (from tensorflow-cpu==2.6.0->-r requirements.txt (line 10)) (2.15.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.37.0 in /workspace/.pip-modules/lib/python3.8/site-packages (from tensorflow-cpu==2.6.0->-r requirements.txt (line 10)) (1.67.0)\n",
            "Requirement already satisfied: entrypoints in /workspace/.pip-modules/lib/python3.8/site-packages (from altair<5->-r requirements.txt (line 13)) (0.4)\n",
            "Requirement already satisfied: jinja2 in /home/gitpod/.pyenv/versions/3.8.18/lib/python3.8/site-packages (from altair<5->-r requirements.txt (line 13)) (3.1.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /home/gitpod/.pyenv/versions/3.8.18/lib/python3.8/site-packages (from altair<5->-r requirements.txt (line 13)) (4.23.0)\n",
            "Requirement already satisfied: toolz in /workspace/.pip-modules/lib/python3.8/site-packages (from altair<5->-r requirements.txt (line 13)) (1.0.0)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /home/gitpod/.pyenv/versions/3.8.18/lib/python3.8/site-packages (from jsonschema>=3.0->altair<5->-r requirements.txt (line 13)) (6.4.5)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/gitpod/.pyenv/versions/3.8.18/lib/python3.8/site-packages (from jsonschema>=3.0->altair<5->-r requirements.txt (line 13)) (2023.12.1)\n",
            "Requirement already satisfied: pkgutil-resolve-name>=1.3.10 in /home/gitpod/.pyenv/versions/3.8.18/lib/python3.8/site-packages (from jsonschema>=3.0->altair<5->-r requirements.txt (line 13)) (1.3.10)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /home/gitpod/.pyenv/versions/3.8.18/lib/python3.8/site-packages (from jsonschema>=3.0->altair<5->-r requirements.txt (line 13)) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /home/gitpod/.pyenv/versions/3.8.18/lib/python3.8/site-packages (from jsonschema>=3.0->altair<5->-r requirements.txt (line 13)) (0.20.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/gitpod/.pyenv/versions/3.8.18/lib/python3.8/site-packages (from jinja2->altair<5->-r requirements.txt (line 13)) (2.1.5)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /workspace/.pip-modules/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow-cpu==2.6.0->-r requirements.txt (line 10)) (2.35.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /workspace/.pip-modules/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow-cpu==2.6.0->-r requirements.txt (line 10)) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /workspace/.pip-modules/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow-cpu==2.6.0->-r requirements.txt (line 10)) (3.7)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /home/gitpod/.pyenv/versions/3.8.18/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow-cpu==2.6.0->-r requirements.txt (line 10)) (75.2.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /workspace/.pip-modules/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow-cpu==2.6.0->-r requirements.txt (line 10)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /workspace/.pip-modules/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow-cpu==2.6.0->-r requirements.txt (line 10)) (3.0.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/gitpod/.pyenv/versions/3.8.18/lib/python3.8/site-packages (from requests->streamlit==0.85.0->-r requirements.txt (line 7)) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/gitpod/.pyenv/versions/3.8.18/lib/python3.8/site-packages (from requests->streamlit==0.85.0->-r requirements.txt (line 7)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/gitpod/.pyenv/versions/3.8.18/lib/python3.8/site-packages (from requests->streamlit==0.85.0->-r requirements.txt (line 7)) (2.2.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /workspace/.pip-modules/lib/python3.8/site-packages (from gitpython->streamlit==0.85.0->-r requirements.txt (line 7)) (4.0.11)\n",
            "Requirement already satisfied: backports.zoneinfo in /workspace/.pip-modules/lib/python3.8/site-packages (from tzlocal->streamlit==0.85.0->-r requirements.txt (line 7)) (0.2.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /workspace/.pip-modules/lib/python3.8/site-packages (from gitdb<5,>=4.0.1->gitpython->streamlit==0.85.0->-r requirements.txt (line 7)) (5.0.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /workspace/.pip-modules/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow-cpu==2.6.0->-r requirements.txt (line 10)) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /workspace/.pip-modules/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow-cpu==2.6.0->-r requirements.txt (line 10)) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /workspace/.pip-modules/lib/python3.8/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard~=2.6->tensorflow-cpu==2.6.0->-r requirements.txt (line 10)) (2.0.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /home/gitpod/.pyenv/versions/3.8.18/lib/python3.8/site-packages (from importlib-resources>=1.4.0->jsonschema>=3.0->altair<5->-r requirements.txt (line 13)) (3.20.2)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /home/gitpod/.pyenv/versions/3.8.18/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow-cpu==2.6.0->-r requirements.txt (line 10)) (8.5.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /workspace/.pip-modules/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow-cpu==2.6.0->-r requirements.txt (line 10)) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /workspace/.pip-modules/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard~=2.6->tensorflow-cpu==2.6.0->-r requirements.txt (line 10)) (3.2.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "!pwd\n",
        "%pip install -r requirements.txt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "# Collect Data via Kaggle API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Install Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle==1.5.12 in /workspace/.pip-modules/lib/python3.8/site-packages (1.5.12)\n",
            "Requirement already satisfied: six>=1.10 in /workspace/.pip-modules/lib/python3.8/site-packages (from kaggle==1.5.12) (1.15.0)\n",
            "Requirement already satisfied: certifi in /home/gitpod/.pyenv/versions/3.8.18/lib/python3.8/site-packages (from kaggle==1.5.12) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil in /home/gitpod/.pyenv/versions/3.8.18/lib/python3.8/site-packages (from kaggle==1.5.12) (2.9.0.post0)\n",
            "Requirement already satisfied: requests in /home/gitpod/.pyenv/versions/3.8.18/lib/python3.8/site-packages (from kaggle==1.5.12) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /workspace/.pip-modules/lib/python3.8/site-packages (from kaggle==1.5.12) (4.66.5)\n",
            "Requirement already satisfied: python-slugify in /workspace/.pip-modules/lib/python3.8/site-packages (from kaggle==1.5.12) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /home/gitpod/.pyenv/versions/3.8.18/lib/python3.8/site-packages (from kaggle==1.5.12) (2.2.3)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /workspace/.pip-modules/lib/python3.8/site-packages (from python-slugify->kaggle==1.5.12) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/gitpod/.pyenv/versions/3.8.18/lib/python3.8/site-packages (from requests->kaggle==1.5.12) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/gitpod/.pyenv/versions/3.8.18/lib/python3.8/site-packages (from requests->kaggle==1.5.12) (3.10)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# confirm kaggle installed if not already\n",
        "%pip install kaggle==1.5.12"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Allow Kaggle configuration using auth JSON setting config dir to current dir."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.environ['KAGGLE_CONFIG_DIR'] = os.getcwd()\n",
        "! chmod 600 kaggle.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set Kaggle Dataset and download it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading cherry-leaves.zip to inputs/cherry-leaves-dataset\n",
            " 95%|███████████████████████████████████▉  | 52.0M/55.0M [00:02<00:00, 18.0MB/s]\n",
            "100%|██████████████████████████████████████| 55.0M/55.0M [00:02<00:00, 20.0MB/s]\n"
          ]
        }
      ],
      "source": [
        "KaggleDatasetPath = \"codeinstitute/cherry-leaves\"\n",
        "DestinationFolder = \"inputs/cherry-leaves-dataset\"  # creates new dir/dir\n",
        "! kaggle datasets download -d {KaggleDatasetPath} -p {DestinationFolder}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Unzip downloaded dataset file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "import zipfile\n",
        "\n",
        "try:\n",
        "    with zipfile.ZipFile(DestinationFolder + '/cherry-leaves.zip', 'r') as zip_ref:\n",
        "        zip_ref.extractall(DestinationFolder)\n",
        "except Exception as e:\n",
        "    print(e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Delete redundant zip file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.remove(DestinationFolder + '/cherry-leaves.zip')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY3l0-AxO93d"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFQo3ycuO-v6"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Cleaning\n",
        "\n",
        "* remove any non-image files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def remove_non_image_files(my_data_dir):\n",
        "    print('Removing non image files...\\n')\n",
        "    image_extension = ('.png', '.jpg', 'jpeg')\n",
        "    folders = os.listdir(my_data_dir)\n",
        "    for folder in folders:\n",
        "        files = os.listdir(f'{my_data_dir}/{folder}')\n",
        "        # print files\n",
        "        non_image = []\n",
        "        image_count = []\n",
        "        for given_file in files:\n",
        "            try:\n",
        "                if not given_file.lower().endswith(image_extension):\n",
        "                    file_location = f'{my_data_dir}/{folder}/{given_file}'\n",
        "                    os.remove(file_location) # remove non image file\n",
        "                    non_image.append(1)\n",
        "                else:\n",
        "                    image_count.append(1)\n",
        "                    pass\n",
        "            except Exception as e:\n",
        "                print(e)\n",
        "\n",
        "        print(f'Folder: {folder} has - {len(image_count)} image files')\n",
        "        print(f'Folder: {folder} has - {len(non_image)} non image files, which have been removed')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Removing non image files...\n",
            "\n",
            "[Errno 21] Is a directory: 'inputs/cherry-leaves-dataset/cherry-leaves/test/healthy'\n",
            "[Errno 21] Is a directory: 'inputs/cherry-leaves-dataset/cherry-leaves/test/powdery_mildew'\n",
            "Folder: test has - 0 image files\n",
            "Folder: test has - 0 non image files, which have been removed\n",
            "[Errno 21] Is a directory: 'inputs/cherry-leaves-dataset/cherry-leaves/train/healthy'\n",
            "[Errno 21] Is a directory: 'inputs/cherry-leaves-dataset/cherry-leaves/train/powdery_mildew'\n",
            "Folder: train has - 0 image files\n",
            "Folder: train has - 0 non image files, which have been removed\n",
            "[Errno 21] Is a directory: 'inputs/cherry-leaves-dataset/cherry-leaves/validation/healthy'\n",
            "[Errno 21] Is a directory: 'inputs/cherry-leaves-dataset/cherry-leaves/validation/powdery_mildew'\n",
            "Folder: validation has - 0 image files\n",
            "Folder: validation has - 0 non image files, which have been removed\n",
            "Folder: healthy has - 2104 image files\n",
            "Folder: healthy has - 0 non image files, which have been removed\n",
            "Folder: powdery_mildew has - 2104 image files\n",
            "Folder: powdery_mildew has - 0 non image files, which have been removed\n"
          ]
        }
      ],
      "source": [
        "remove_non_image_files('inputs/cherry-leaves-dataset/cherry-leaves')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Split train validation test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "import joblib\n",
        "\n",
        "def split_train_validation_test_images(my_data_dir, train_set_ratio, validation_set_ratio, test_set_ratio):\n",
        "\n",
        "    # confirm ratios total 1.0\n",
        "    if train_set_ratio + validation_set_ratio + test_set_ratio != 1.0:\n",
        "        print('Ratios should total 1.0.')\n",
        "        print('You entered:\\n')\n",
        "        print(f'Train radio: {train_set_ratio}')\n",
        "        print(f'Validation radio: {validation_set_ratio}')\n",
        "        print(f'Test radio: {test_set_ratio}')\n",
        "        return \n",
        "\n",
        "    # get classes labels\n",
        "    labels = os.listdir(my_data_dir)  # expect only folder name\n",
        "    if 'test' in labels:\n",
        "        pass\n",
        "    else:\n",
        "        try:\n",
        "            # create train, test folders with classes labels sub-folder\n",
        "            for folder in ['train', 'validation', 'test']:\n",
        "                for label in labels:\n",
        "                    os.makedirs(name=f'{my_data_dir}/{folder}/{label}')\n",
        "            \n",
        "            for label in labels:\n",
        "\n",
        "                files = os.listdir(f'{my_data_dir}/{label}')\n",
        "                random.shuffle(files)\n",
        "\n",
        "                train_set_files_qty = int(len(files) * train_set_ratio)\n",
        "                validation_set_files_qty = int(len(files) * validation_set_ratio)\n",
        "\n",
        "                count = 1\n",
        "                for file_name in files:\n",
        "                    if count <= train_set_files_qty:\n",
        "                        # move given file to train set\n",
        "                        shutil.move(f'{my_data_dir}/{label}/{file_name}',\n",
        "                                    f'{my_data_dir}/train/{label}/{file_name}')\n",
        "                    elif count <= (train_set_files_qty + validation_set_files_qty):\n",
        "                        # move a given file to the validation set\n",
        "                        shutil.move(f'{my_data_dir}/{label}/{file_name}',\n",
        "                                    f'{my_data_dir}/validation/{label}/{file_name}')\n",
        "                    else:\n",
        "                        # move given file to test set\n",
        "                        shutil.move(f'{my_data_dir}/{label}/{file_name}',\n",
        "                                    f'{my_data_dir}/test/{label}/{file_name}')\n",
        "                    \n",
        "                    count += 1\n",
        "\n",
        "                os.rmdir(f'{my_data_dir}/{label}')\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "    print('Done!')\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Conventional ratios will be followed here:\n",
        "* The training set is divided into a 0.70 ratio of data.\n",
        "* The validation set is divided into a 0.10 ratio of data.\n",
        "* The test set is divided into a 0.20 ratio of data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done!\n"
          ]
        }
      ],
      "source": [
        "split_train_validation_test_images(\n",
        "    my_data_dir='inputs/cherry-leaves-dataset/cherry-leaves',\n",
        "    train_set_ratio=0.7,\n",
        "    validation_set_ratio=0.1,\n",
        "    test_set_ratio=0.2\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltNetd085qHf"
      },
      "source": [
        "# Prepare for push to repository"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Ensure the downloaded files are not added to repo by including in .gitinore file (they can be redownloaded in the workspace if neccessary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "!echo \"/inputs/cherry-leaves-dataset/cherry-leaves/\" >> .gitignore  # add if doesn't already exists (bash)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Confirm included in .gitignore before pushing to repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "core.Microsoft*\n",
            "core.mongo*\n",
            "core.python*\n",
            "env.py\n",
            "__pycache__/\n",
            "*.py[cod]\n",
            "node_modules/\n",
            ".github/\n",
            "cloudinary_python.txt\n",
            "kaggle.json-e \n",
            "/inputs/cherry-leaves-dataset/cherry-leaves/\n",
            "/inputs/cherry-leaves-dataset/cherry-leaves/\n"
          ]
        }
      ],
      "source": [
        "cat .gitignore"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now commit to repository."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "All files are now prepared for data exploration.\n",
        "Only images are in the dataset according to defined file types. "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "8b8334dab9339717f727a1deaf837b322d7a41c20d15cc86be99a8e69ceec8ce"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 64-bit ('3.8.12': pyenv)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
